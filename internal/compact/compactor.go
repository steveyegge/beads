package compact

import (
	"context"
	"errors"
	"fmt"
	"sync"

	"github.com/steveyegge/beads/internal/types"
)

const (
	defaultConcurrency = 5
)

// Config holds configuration for the compaction process.
type Config struct {
	APIKey       string
	Concurrency  int
	DryRun       bool
	AuditEnabled bool
	Actor        string
}

// Compactor handles issue compaction using AI summarization.
type Compactor struct {
	store      CompactableStore
	summarizer summarizer
	config     *Config
}

// CompactableStore defines the storage interface required for compaction.
// This interface can be implemented by any storage backend (SQLite, Dolt, etc.)
// that wants to support the compaction feature.
type CompactableStore interface {
	CheckEligibility(ctx context.Context, issueID string, tier int) (bool, string, error)
	GetIssue(ctx context.Context, issueID string) (*types.Issue, error)
	UpdateIssue(ctx context.Context, issueID string, updates map[string]interface{}, actor string) error
	ApplyCompaction(ctx context.Context, issueID string, tier int, originalSize int, compactedSize int, commitHash string) error
	AddComment(ctx context.Context, issueID, actor, comment string) error
	MarkIssueDirty(ctx context.Context, issueID string) error
}

type summarizer interface {
	SummarizeTier1(ctx context.Context, issue *types.Issue) (string, error)
}

// New creates a new Compactor instance with the given configuration.
// The store parameter must implement CompactableStore interface.
func New(store CompactableStore, apiKey string, config *Config) (*Compactor, error) {
	if config == nil {
		config = &Config{
			Concurrency: defaultConcurrency,
		}
	}
	if config.Concurrency <= 0 {
		config.Concurrency = defaultConcurrency
	}
	if apiKey != "" {
		config.APIKey = apiKey
	}

	var haikuClient summarizer
	var err error
	if !config.DryRun {
		haikuClient, err = NewHaikuClient(config.APIKey)
		if err != nil {
			if errors.Is(err, ErrAPIKeyRequired) {
				config.DryRun = true
			} else {
				return nil, fmt.Errorf("failed to create Haiku client: %w", err)
			}
		}
	}
	if hc, ok := haikuClient.(*HaikuClient); ok {
		hc.auditEnabled = config.AuditEnabled
		hc.auditActor = config.Actor
	}

	return &Compactor{
		store:      store,
		summarizer: haikuClient,
		config:     config,
	}, nil
}

// CompactTier1 compacts a single issue at Tier 1 (basic summarization).
func (c *Compactor) CompactTier1(ctx context.Context, issueID string) error {
	eligible, reason, err := c.store.CheckEligibility(ctx, issueID, 1)
	if err != nil {
		return fmt.Errorf("failed to check eligibility: %w", err)
	}
	if !eligible {
		if reason == "" {
			reason = "not eligible"
		}
		return fmt.Errorf("issue not eligible for compaction: %s", reason)
	}

	issue, err := c.store.GetIssue(ctx, issueID)
	if err != nil {
		return fmt.Errorf("failed to fetch issue: %w", err)
	}

	// Calculate original size
	originalSize := len(issue.Description) + len(issue.Design) + len(issue.Notes) + len(issue.AcceptanceCriteria)

	if c.config.DryRun {
		return fmt.Errorf("dry-run enabled: compaction skipped")
	}

	// Get summary from AI
	if c.summarizer == nil {
		return fmt.Errorf("summarizer not configured")
	}
	summary, err := c.summarizer.SummarizeTier1(ctx, issue)
	if err != nil {
		return fmt.Errorf("failed to summarize: %w", err)
	}

	if len(summary) >= originalSize {
		comment := fmt.Sprintf("Tier 1 compaction skipped: summary is not smaller (original %d bytes, summary %d bytes)",
			originalSize, len(summary))
		_ = c.store.AddComment(ctx, issueID, "compactor", comment)
		return fmt.Errorf("compaction would increase size")
	}

	// Update issue with summarized content
	updates := map[string]interface{}{
		"description":        summary,
		"design":             "",
		"notes":              "",
		"acceptance_criteria": "",
	}
	if err := c.store.UpdateIssue(ctx, issueID, updates, "compactor"); err != nil {
		return fmt.Errorf("failed to update issue: %w", err)
	}

	// Record compaction metadata
	compactedSize := len(summary)
	commitHash := GetCurrentCommitHash()
	if err := c.store.ApplyCompaction(ctx, issueID, 1, originalSize, compactedSize, commitHash); err != nil {
		return fmt.Errorf("failed to apply compaction metadata: %w", err)
	}

	// Add comment about compaction
	saved := originalSize - compactedSize
	percent := 0.0
	if originalSize > 0 {
		percent = float64(saved) / float64(originalSize) * 100
	}
	comment := fmt.Sprintf("Tier 1 compaction applied. Original size: %d bytes, Compacted size: %d bytes (saved %d bytes, %.1f%% reduction)",
		originalSize, compactedSize, saved, percent)
	if err := c.store.AddComment(ctx, issueID, "compactor", comment); err != nil {
		return fmt.Errorf("failed to add compaction comment: %w", err)
	}

	// Mark dirty for export
	if err := c.store.MarkIssueDirty(ctx, issueID); err != nil {
		return fmt.Errorf("failed to mark dirty: %w", err)
	}

	return nil
}

// BatchResult holds the result of a single issue compaction in a batch.
type BatchResult struct {
	IssueID       string
	OriginalSize  int
	CompactedSize int
	Err           error
}

// CompactTier1Batch compacts multiple issues at Tier 1 concurrently.
func (c *Compactor) CompactTier1Batch(ctx context.Context, issueIDs []string) ([]BatchResult, error) {
	results := make([]BatchResult, len(issueIDs))
	sem := make(chan struct{}, c.config.Concurrency)
	var wg sync.WaitGroup

	for i, id := range issueIDs {
		wg.Add(1)
		go func(idx int, issueID string) {
			defer wg.Done()
			sem <- struct{}{}
			defer func() { <-sem }()

			// Get issue to calculate original size
			issue, err := c.store.GetIssue(ctx, issueID)
			if err != nil {
				results[idx] = BatchResult{IssueID: issueID, Err: err}
				return
			}

			originalSize := len(issue.Description) + len(issue.Design) + len(issue.Notes) + len(issue.AcceptanceCriteria)

			err = c.CompactTier1(ctx, issueID)
			if err != nil {
				results[idx] = BatchResult{IssueID: issueID, OriginalSize: originalSize, Err: err}
				return
			}

			// Get updated issue to calculate compacted size
			issueAfter, _ := c.store.GetIssue(ctx, issueID)
			compactedSize := 0
			if issueAfter != nil {
				compactedSize = len(issueAfter.Description)
			}

			results[idx] = BatchResult{
				IssueID:       issueID,
				OriginalSize:  originalSize,
				CompactedSize: compactedSize,
			}
		}(i, id)
	}

	wg.Wait()
	return results, nil
}

// CompactTier1Single compacts a single issue at Tier 1 and returns detailed results.
// Deprecated: Use CompactTier1 instead. This method is kept for backward compatibility.
func (c *Compactor) CompactTier1Single(ctx context.Context, issueID string) (*BatchResult, error) {
	issue, err := c.store.GetIssue(ctx, issueID)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch issue: %w", err)
	}

	originalSize := len(issue.Description) + len(issue.Design) + len(issue.Notes) + len(issue.AcceptanceCriteria)

	if err := c.CompactTier1(ctx, issueID); err != nil {
		return &BatchResult{IssueID: issueID, OriginalSize: originalSize, Err: err}, nil
	}

	issueAfter, _ := c.store.GetIssue(ctx, issueID)
	compactedSize := 0
	if issueAfter != nil {
		compactedSize = len(issueAfter.Description)
	}

	return &BatchResult{
		IssueID:       issueID,
		OriginalSize:  originalSize,
		CompactedSize: compactedSize,
	}, nil
}

// CompactTier2 is a placeholder for future Tier 2 compaction.
// Tier 2 would involve more aggressive summarization and archival.
func (c *Compactor) CompactTier2(ctx context.Context, issueID string) error {
	// Get the issue
	issue, err := c.store.GetIssue(ctx, issueID)
	if err != nil {
		return fmt.Errorf("failed to get issue: %w", err)
	}

	// Calculate original size
	originalSize := len(issue.Description) + len(issue.Design) + len(issue.Notes) + len(issue.AcceptanceCriteria)

	// For now, just apply the metadata update without actual summarization
	if err := c.store.ApplyCompaction(ctx, issueID, 2, originalSize, originalSize, ""); err != nil {
		return fmt.Errorf("failed to apply compaction metadata: %w", err)
	}

	// Mark dirty for export
	if err := c.store.MarkIssueDirty(ctx, issueID); err != nil {
		return fmt.Errorf("failed to mark dirty: %w", err)
	}

	return nil
}
